{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hacettepe University Computer Engineering Department\n",
    "#### BBM-409 Machine Learning Lab - Assignment 1\n",
    "#### Detection of Fake News\n",
    "##### Name/Surname: Furkan Çağlar Gülmez\n",
    "##### Id: 21527027 \n",
    "##### Advisors: Dr. Aykut Erdem, T.A. Necva Bölücü\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **MLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a-)** Suppose you have N samples x1,x2.....xN from a univariate normal distribution with unknown mean μ and known variance σ2. Derive the MLE estimator for the mean μ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _____ <img src=\"mle1.jpg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b-)** Consider a dataset (xn, cn), n = 1, ..., N of binary attributes, xni ∈ 0, 1, i = 1, ..., D and associated class label cn. The number of datapoints from class c = 0 is denoted n0 and the number from class c = 1 is denoted n1. Estimate p(xi = 1|c) ≡ θic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____ <img src=\"ml4.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c-)** Suppose that X is a discrete rrandom variable with the following probability mass function: where 0 ≤ θ ≤ 1 is a parameter. The following 10 independent ob- servations were taken from such a distribution: (3,0,2,1,3,2,1,0,2,1). What is the maximum likelihood estimate of θ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____ <img src=\"mle2.jpg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A psychologist does a small survey on ’happiness’. Each respondent provides a vector with entries 1 or 0 corresponding to whether they answer ’yes’ to a question or ’no’, respectively. The question vector has attributes\n",
    "x = (rich, married, healthy)\n",
    "Thus, a response (1, 0, 1) would indicate that the respondent was ’rich’, ’unmarried’, ’healthy’. In addition, each respondent gives a value c = 1 if they are content with their lifestyle, and c = 0 if they are not. The following responses were obtained from people who claimed also to be ’content’: (1, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 1) and for ’not content’: (0, 0, 0), (1, 0, 0), (0, 0, 1), (0, 1, 0), (0, 0, 0) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using Naive Bayes, what is the probability that a person who is ’not rich’, ’married’ and ’healthy’ is ’content’?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    P(content) = 4/9     P(not_content) = 5/9\n",
    "    P(content|0 1 1) = P(0 1 1|content) * P(content)\n",
    "    P(not rich | content) = 1/4\n",
    "    P(married | content) = 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the probability that a person who is ’not rich’ and ’married’ is ’content’? (That is, we do not know whether ot not they are ’healthy’.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    P(health|content) = 3/4\n",
    "    After the calculations that is above, \n",
    "    P(content | 0 1 1) = 1/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Detection of Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Used Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), stop_words=[])\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Features:\\n\", vectorizer.get_feature_names(),\n",
    "      '\\nConverse mapping:\\n', X.toarray(),\n",
    "      '\\nFeature Document\\'s Index is:', vectorizer.vocabulary_.get('document'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Features:\n",
    "     ['and', 'document', 'first', 'is', 'one', \n",
    "     'second', 'the', 'third', 'this'] \n",
    "     \n",
    "    Converse mapping:\n",
    "     [[0 1 1 1 0 0 1 0 1]\n",
    "      [0 1 0 1 0 2 1 0 1]\n",
    "      [1 0 0 0 1 0 1 1 0]\n",
    "      [0 1 1 1 0 0 1 0 1]] \n",
    "      \n",
    "    Feature Document's Index is: 1\n",
    "    \n",
    "     So that we see that, the word 'document' is repeated \n",
    "     three times which are line first, second and last one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Numpy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. The ancestor of NumPy, Numeric, was originally created by Jim Hugunin with contributions from several other developers. In 2005, Travis Oliphant created NumPy by incorporating features of the competing Numarray into Numeric, with extensive modifications. NumPy is open-source software and has many contributors.\n",
    "I used numpy when I calculate cosine similarity and pearson correlation of two vectors. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "X = np.array(np.sum(X, axis=0))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The resulting array is -> [1 3 2 3 1 2 4 1 3] \n",
    "    This array says each word's count in the document.\n",
    "    For example:  X[ vectorizer.vocabulary_.get('document') ] \n",
    "    == X [ 1 ] which is 3, that means that there is three word 'document'\n",
    "    in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Math**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For basic logarithmic calculations, I used math library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets, Filtering, Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **clean_fake-Train.txt & clean_real-train.txt:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1298 fake news headlines (which mostly include headlines of articles classified as bi- ased etc.) and 1968 real news headlines, where the fake news headlines are from https://www.kaggle.com/mrisdal/fake-news/data and real news headlines are from https://www.kaggle.com/therohk/million-headlines have been compiled. Data is cleaned by removing words from fake news titles that are not a part of the headline, removing special characters from the headlines, and restricting real news headlines to those after October 2016 containing the word trump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **test.csv:** Dataset contains a new headline and its label to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Firstly, I tried to write my own stemmer. Because nltk's stemmer is not enough for cleans prefix of words.\n",
    "But then I realized that when we clean data, accuracy is decreased so that I gave up writing my own stemmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I did this filterings 8950 users and 25800 ratings remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def read(path):\n",
    "    file = open(path, 'r')\n",
    "    return [line.rstrip('\\n') for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* read() function reads file, takes file path as a parameter and return an array which holds all line, **each line is one string**\n",
    "* I also read the file with nltk's snowball stemmer and lemmatizer but I realized that when I let the code lemmatize or stem accuracy is decreased. Then I decided not to use it. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_vectorizer(arr, ngram):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(ngram, ngram))\n",
    "    X = vectorizer.fit_transform(arr).toarray()\n",
    "    X = np.array(np.sum(X, axis=0))\n",
    "    d1 = vectorizer.fit(arr).vocabulary_\n",
    "    return X, d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create_vectorizer() function takes an array and an int value as parameters.\n",
    "    * array is which is returned from read() function\n",
    "    * int value is to decide unigram or bigram\n",
    "    * if ngram == 1 perform unigram, if ngram == 2 perform bigram\n",
    "* returns \n",
    "    * a numpy array which holds count of words \n",
    "    * dictionary which holds word's indexes in numpy array "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def naive_bayes(sentences_tuple, arr1, arr2, d1, d2, fake_freq, real_freq, ngram):\n",
    "    for pair in sentences_tuple:\n",
    "        real_prob = 0\n",
    "        fake_prob = 0\n",
    "        prediction = ''\n",
    "        for word in pair[0].split(' '):\n",
    "            fake_prob += math.log(fake_probability(word, arr1, arr2, d1, d2))\n",
    "            real_prob += math.log(real_probability(word, arr1, arr2, d1, d2))\n",
    "        fake_prob += math.log(fake_freq)\n",
    "        real_prob += math.log(real_freq)\n",
    "\n",
    "        if real_prob > fake_prob:\n",
    "            prediction = \"real\"\n",
    "\n",
    "        else:\n",
    "            prediction = \"fake\"\n",
    "            \n",
    "        if prediction == pair[1]:\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1\n",
    "    accuracy = 100 * (true / (true + false))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parameters:\n",
    "    * sentences_tuple  : test data to predict\n",
    "    *            arr1  : real data train\n",
    "    *            arr2  : fake data train\n",
    "    *              d1  : fake data dictionary\n",
    "    *              d2  : real data dictionary\n",
    "    *       fake_freq  : (fake line count) / (fake + real)\n",
    "    *       real_freq  : (real line count) / (fake + real)\n",
    "    *           ngram  : if 1 -> unigram, if 2 -> bigram\n",
    "\n",
    "\n",
    "* returns:\n",
    "    * accuracy of trained model\n",
    "\n",
    "\n",
    "* function basicaly calculates naive bayes with all words in news' headlines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "#### Understanding the data\n",
    "The data-set consists of real and fake news headlines related to the recent U.S elections, with about 2000 real headline and about 1300 fake headlines. By looking at the number of word occurrences we see that the words used the most for both real and fake headlines are mostly similar. We can also see that many of those words are stop-words (”of”, ”for”, ”on” ... etc). I have listed below three examples that I believe will help in classifying the headlines as real or fake:\n",
    "\n",
    "**Real Headlines' Words (Unigram)**\n",
    "\n",
    "\n",
    "* korea    : \n",
    "    * Real Count: 62  \n",
    "    * Fake Count: 0 \n",
    "    * Rate: 1.000\n",
    "\n",
    "\n",
    "* ban      : \n",
    "    * Real Count: 65  \n",
    "    * Fake Count: 1 \n",
    "    * Rate: 0.985\n",
    "\n",
    "\n",
    "* trumps   : \n",
    "    * Real Count: 184 \n",
    "    * Fake Count: 4 \n",
    "    * Rate: 0.979\n",
    "\n",
    "\n",
    "**Fake Headlines' Words (Unigram)**\n",
    "\n",
    "\n",
    "* just     : \n",
    "    * Real Count: 8 \n",
    "    * Fake Count: 67\n",
    "    * Rate: 0.893\n",
    "\n",
    "\n",
    "* you      : \n",
    "    * Real Count: 7 \n",
    "    * Fake Count: 55 \n",
    "    * Rate: 0.887\n",
    "\n",
    "\n",
    "* hillary  : \n",
    "    * Real Count: 21 \n",
    "    * Fake Count: 126 \n",
    "    * Rate: 0.857\n",
    "\n",
    "\n",
    "**Real Headlines' Words (Bigram)**\n",
    "\n",
    "\n",
    "* north korea   : \n",
    "    * Real Count: 60 \n",
    "    * Fake Count: 0 \n",
    "    * Rate: 1.000\n",
    "\n",
    "\n",
    "* donald trumps : \n",
    "    * Real Count: 73 \n",
    "    * Fake Count: 1 \n",
    "    * Rate: 0.986\n",
    "\n",
    "\n",
    "* trump says    : \n",
    "    * Real Count: 84 \n",
    "    * Fake Count: 12 \n",
    "    * Rate: 0.875\n",
    "\n",
    "\n",
    "**Fake Headlines' Words (Bigram)**\n",
    "\n",
    "\n",
    "* hillary clinton : \n",
    "    * Real Count: 19 \n",
    "    * Fake Count: 40 \n",
    "    * Rate: 0.678\n",
    "\n",
    "\n",
    "* trump to        : \n",
    "    * Real Count: 29 \n",
    "    * Fake Count: 27 \n",
    "    * Rate: 0.482\n",
    "\n",
    "\n",
    "* trump and       :\n",
    "    * Real Count: 33 \n",
    "    * Fake Count: 28 \n",
    "    * Rate: 0.459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "#### (a) Analyzing effect of the words on prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **real presence**\n",
    "\n",
    "    * korea     : 0.9999984706460329 \n",
    "    * travel    : 0.999998161452981\n",
    "    * turnbull  : 0.9999979567576792 \n",
    "    * australia : 0.9999963593007417\n",
    "    * refugee   : 0.999995744141818\n",
    "    * paris     : 0.9999957441414531\n",
    "    * debate    : 0.9999948834115259 \n",
    "    * tax       : 0.9999948834110846\n",
    "    * congress  : 0.9999945147023779 \n",
    "    * defends   : 0.9999945147023779\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **real absence**\n",
    "\n",
    "    * star : 0.6022727278336234 \n",
    "    * black : 0.6022727277070424\n",
    "    * breaking : 0.6022727276231271\n",
    "    * u: 0.60227272758131\n",
    "    * soros : 0.6022727275395864\n",
    "    * won : 0.6022727275395864\n",
    "    * woman : 0.6022727274564178 \n",
    "    * homeless : 0.6022727273736188\n",
    "    * info : 0.6022727273323573\n",
    "    * daily : 0.6022727272911872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **fake presence**\n",
    "\n",
    "    * star : 0.6503072189992896 \n",
    "    * black : 0.613751194679261\n",
    "    * breaking : 0.5849902550380285), \n",
    "    * u : 0.5689972950641514\n",
    "    * soros : 0.5517624714313497 \n",
    "    * won : 0.551762461072533\n",
    "    * woman : 0.5129396870889822 \n",
    "    * homeless : 0.4669787763834351\n",
    "    * info : 0.44067670120589464 \n",
    "    * daily : 0.41171181939951795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **fake absence**\n",
    "\n",
    "    * trump : 0.39782056097\n",
    "    * donald : 0.39773751613\n",
    "    * to : 0.397730715197426\n",
    "    * trumps : 0.39772921604\n",
    "    * in : 0.397729024883772\n",
    "    * us : 0.397729017719476\n",
    "    * on : 0.397728782919198\n",
    "    * says : 0.3977287493500\n",
    "    * of : 0.397728657084839\n",
    "    * for : 0.39772865663446"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **As we see notice that some words presence have a bigger effect on classifying headline than the effect of words being absent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**without stopwords the list is below**\n",
    "\n",
    "* **real presence**\n",
    "\n",
    "    * korea:  0.99999833027299      \n",
    "    * travel:  0.999997992\n",
    "    * turnbull: 0.99999776923       \n",
    "    * australia: 0.9999960\n",
    "    * refugee: 0.999995353595       \n",
    "    * paris: 0.99999535359\n",
    "    * debate: 0.9999944138877       \n",
    "    * tax: 0.9999944138872\n",
    "    * congress: 0.99999401134       \n",
    "    * defends: 0.999994011\n",
    "\n",
    "* **real absence**\n",
    "\n",
    "    * star: 0.602272727919815       \n",
    "    * black: 0.60227272778\n",
    "    * breaking: 0.60227272768       \n",
    "    * u: 0.602272727641606\n",
    "    * won: 0.6022727275956004       \n",
    "    * soros: 0.60227272759\n",
    "    * woman: 0.60227272750389       \n",
    "    * homeless: 0.60227272\n",
    "    * info: 0.602272727367098       \n",
    "    * daily: 0.60227272732\n",
    "    \n",
    "* **fake presence**\n",
    "\n",
    "    * star: 0.669992714999326       \n",
    "    * black: 0.63433831479\n",
    "    * breaking: 0.60612716074       \n",
    "    * u: 0.590378685282129\n",
    "    * soros: 0.57335807383263      \n",
    "    * won: 0.5733580635116\n",
    "    * woman: 0.53482944641718       \n",
    "    * homeless: 0.48887646,\n",
    "    * info: 0.462411309851494       \n",
    "    * daily: 0.43312440248\n",
    "\n",
    "* **fake absence**\n",
    "\n",
    "    * trump: 0.39782101324822       \n",
    "    * donald: 0.3977376125\n",
    "    * trumps: 0.3977292352835       \n",
    "    * says: 0.397728763873\n",
    "    * clinton: 0.397727954887       \n",
    "    * election: 0.39772794\n",
    "    * ban: 0.3977278461398317       \n",
    "    * korea: 0.39772780133\n",
    "    * north: 0.39772780128976       \n",
    "    * president: 0.3977277"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It would make sense to remove stop words as they are necessary words used in all sentences and they do not add any value in determining if a headline is real or fake. On the other hand It would make sense to keep them, as they help in identifying relations between headlines. For example if fake headlines has some specific patterns related to those stop words, we can detect this pattern and it will help us categorize the headlines. This is especially useful if many of the headlines have the same origin.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Predictions & Accuracy Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gg.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with no stemmer or lemmatizer accuracies:\n",
    "    * unigram: 85.480\n",
    "    * bigram : 81.800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with snowballstemmer accuracies:\n",
    "    * unigram: 79.754\n",
    "    * bigram : 77.300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with lemmatizer accuracies:\n",
    "    * unigram: 79.141\n",
    "    * bigram : 81.390"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seen in the plot, this is why I decided not to use stemmer or lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1708.07104.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/23850256/how-can-i-pass-a-preprocessor-to-tfidfvectorizer-sklearn-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
